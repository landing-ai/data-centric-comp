# Allows at most 5 submissions per user per period, where period is 24 hours by default.
max_submissions_per_period: 5
refresh_period_seconds: 1

# UUID of the worksheet where prediction and evaluation bundles are created for submissions.
log_worksheet_uuid: '0xf442d1693b224182a6967844faf52249'    

# Configure the tag that participants use to submit to the competition.
# In this example, any bundle with the tag `some-competition-submit` would be
# considered as an official submission.
submission_tag: submit-roman

# Configure how to mimic the submitted prediction bundles. When evaluating a submission, 
# `new` bundle will replace `old` bundle.
# For a machine learning competition, `old` bundle might be the dev set and `new` bundle
# might be the hidden test set.
predict:
  mimic:
    - {new: '0x5d034887a6e24cfd9849790447946ef2', old: '0x1a1aef4f21f24c8e8da471a58c5659b7'}
  tag: predict-roman

# Configure how to evaluate the new prediction bundles.
# In this example, evaluate.py is script that takes in the paths of the test labels and 
# predicted labels and outputs the evaluation results.
evaluate:
  # Essentially
  #     cl run evaluate.py:0x089063eb85b64b239b342405b5ebab57 \
  #            test.json:0x5538cba32e524fad8b005cd19abb9f95 \
  #            predictions.json:{predict}/predictions.json --- \
  #            python evaluate.py test.json predictions.json
  # where {predict} gets filled in with the uuid of the mimicked bundle above.
  dependencies:
  - {child_path: evaluate.py, parent_uuid: '0x654bf5fd29b9453580562304e3f09ea9'}
  - {child_path: test, parent_uuid: '0x5d034887a6e24cfd9849790447946ef2'}
  - {child_path: predictions.json, parent_uuid: '{predict}'}
  command: python evaluate.py predictions.json test
  tag: eval-roman
  metadata:
    request_docker_image: jupyter/tensorflow-notebook

# Define how to extract the scores from the evaluation bundle.
# In this example, result.json is a JSON file outputted from the evaluation step
# with F1 and exact match metrics (e.g. {"f1": 91, "exact_match": 92}).
score_specs:
- {key: '/result.json:accuracy', name: accuracy}