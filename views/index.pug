extends layout

block title
  title Data-Centric ML Competition

block description
  meta(name='description', content='placeholder')

block extralinks
  link(rel='stylesheet', href='/stylesheets/index.css')
  script(async defer src="https://buttons.github.io/buttons.js")

block extrascripts

mixin model_display(group, is_test)
  table.table.performanceTable
    tr
      if is_test
        th Rank
      th Model
      th Accuracy
    - var largest_f1 = Math.max.apply(null, group.map(function (model) { return model.f1; }))
    each model in group
      tr
        if is_test
          td 
            p #{model.rank}
            span.date.label.label-default #{moment.unix(model.date).format('MMM DD, YYYY')}
        td(style="word-break:break-word;")
          | #{model.model_name}
          p.institution #{model.institution}
          if model.link
            a.link(href=model.link) #{model.link}
        td
          if model.f1 == largest_f1
            b #{model.f1.toPrecision(5)}
          else
            | #{model.f1.toPrecision(5)}

block content
  .cover#contentCover
    .container
      .row
        .col-md-5
          .infoCard
            .infoBody
              .infoHeadline
                h2 What is Data-Centric ML?
              p 
                span
                | Here at Landing AI we heavily utilize data-centric approaches to improve the performance of our machine learning models. These include techniques such as fixing incorrect labels or ambiguous labels, adding data for side cases tuning , adding data augmentation techniques, etc. We have seen this data centric approach used widely across AI problems in industry yet scarcely examined in academics, so we created this competition to compare and improve data-centric approaches to improving machine learning model performance.
              p
              .infoHeadline
                h2 Getting Started
              p
                | Here's a few resources to help you get started with the data-centric competition
              p 
                | Download a copy of the dataset 
                ul.list-unstyled
                  li
                    a.btn.actionBtn.inverseBtn(href="http://cs231n.stanford.edu/tiny-imagenet-200.zip", download)
                      | Dataset
              p To evaluate your dataset, we have also made available the evaluation script we will use for official evaluation, along with a sample prediction file that the script will take as input. To run the evaluation, use 
                code
                  | python evaluate.py &lt;path_to_dev&gt; &lt;path_to_predictions&gt;
                |.
                ul.list-unstyled
                  li
                    a.btn.actionBtn.inverseBtn(href="https://worksheets.codalab.org/rest/bundles/0x6b567e1cf2e041ec80d7098f031c5c9e/contents/blob/", download)
                      | Evaluation Script
                  li
                    a.btn.actionBtn.inverseBtn(href="https://worksheets.codalab.org/bundles/0x8731effab84f41b7b874a070e40f61e2/", download)
                      | Sample Dataset File
              p Once you built a dataset that you like, you can submit it to get official scores on dev adn hidden test set.
              a.btn.actionBtn.inverseBtn(href="https://worksheets.codalab.org/worksheets/0x8212d84ca41c4150b555a075b19ccc05/")
                | Submission Tutorial
              .infoHeadline
                h2 Have Questions?
              p 
                | Ask us questions at 
                a(href="mailto:dillon@landing.ai") dillon@landing.ai
                |  and 
                a(href="mailto:lynn.he@deeplearning.ai") lynn.he@deeplearning.ai
                | .
            .infoSubheadline
              include includes/github
        .col-md-7
          .infoCard
            .infoBody
              .infoHeadline
                h2 Leaderboard
              +model_display(test, true)
